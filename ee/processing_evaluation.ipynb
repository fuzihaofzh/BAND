{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "import os\n",
    "import re\n",
    "import requests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = {'train': './dataset/rand/train.csv', \n",
    "         'dev': './dataset/rand/dev.csv',\n",
    "         'test': './dataset/rand/test.csv'}\n",
    "\n",
    "# files = {'train': './dataset/stratified/train.csv', \n",
    "#          'dev': './dataset/stratified/dev.csv',\n",
    "#          'test': './dataset/stratified/test.csv'}\n",
    "\n",
    "replace_to = {'1)  Which infectious disease caused the outbreak?': 'disease',\n",
    "              '2)  In which country is the outbreak taking place?': 'country',\n",
    "              '3)  In which province is the outbreak taking place?': 'province',\n",
    "              '4)  In which city/town is the outbreak taking place?': 'city',\n",
    "              '5)  Check and fill country Geo Code (e.g. 1794299):': 'country Code',\n",
    "              '6)  Check and fill province Geo Code (e.g. 1794299):': 'province Code',\n",
    "              '7) Check and fill city Geo Code (e.g. 1815286):': 'city Code',\n",
    "              '8)  Which virus or bacteria caused the outbreak?': 'virus',\n",
    "              '9)  What symptoms were experienced by the infected victims?': 'symptoms',\n",
    "              '11) What is the type of the victims?': 'victims',\n",
    "              'Text': 'Text'}\n",
    "\n",
    "# replace_to = {'1)  Which infectious disease caused the outbreak?': 'disease',\n",
    "#               '2)  In which country is the outbreak taking place?': 'country',\n",
    "#               '3)  In which province is the outbreak taking place?': 'province',\n",
    "#               '4)  In which city/town is the outbreak taking place?': 'city',\n",
    "#               '8)  Which virus or bacteria caused the outbreak?': 'virus',\n",
    "#               '9)  What symptoms were experienced by the infected victims?': 'symptoms',\n",
    "#               '11) What is the type of the victims?': 'victims',\n",
    "#               'Text': 'Text'}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load original data, filter and save to T5 generation format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['disease', 'country', 'province', 'city', 'country Code', 'province Code', 'city Code', 'virus', 'symptoms', 'victims', 'Text']\n",
      "['disease', 'country', 'province', 'city', 'country Code', 'province Code', 'city Code', 'virus', 'symptoms', 'victims', 'Text']\n",
      "['disease', 'country', 'province', 'city', 'country Code', 'province Code', 'city Code', 'virus', 'symptoms', 'victims', 'Text']\n"
     ]
    }
   ],
   "source": [
    "def process_save(mode, finetune_dir):\n",
    "    df = pd.read_csv(files[mode])\n",
    "    df = df[list(replace_to.keys())].rename(columns=replace_to).fillna(\"[None]\")\n",
    "    values = df.values.tolist()\n",
    "    keys = df.columns.tolist()\n",
    "\n",
    "    print(keys)\n",
    "    inputs = []\n",
    "    targets = []\n",
    "    events = []\n",
    "    for sample_i in values:\n",
    "        inputs.append(\"Extract: \" + sample_i[-1])\n",
    "        tar_i = []\n",
    "        event_i = {}\n",
    "        for r in range(len(keys[:-1])):\n",
    "            tar_i_r = str(sample_i[r]).split(\"; \")\n",
    "            new_tar_i_r = \" [And] \".join(tar_i_r)\n",
    "            new_tar_i_r.replace(\"  \", \" \")\n",
    "            tar_i.append(\"<|{0}|> {1} <|/{0}|>\".format(keys[r], new_tar_i_r))\n",
    "            event_i[keys[r]] = sample_i[r]\n",
    "        targets.append(\" \".join(tar_i))\n",
    "        events.append(event_i)\n",
    "    \n",
    "    with open('{}/{}_input.json'.format(finetune_dir, mode), 'w') as f:\n",
    "        json.dump(inputs, f, indent=4)\n",
    "\n",
    "    with open('{}/{}_target.json'.format(finetune_dir, mode), 'w') as f:\n",
    "        json.dump(targets, f, indent=4)\n",
    "\n",
    "    with open('{}/{}_all.pkl'.format(finetune_dir, mode), 'wb') as f:\n",
    "        pickle.dump({\n",
    "            'input': inputs,\n",
    "            'target': targets,\n",
    "            'all': events\n",
    "        }, f)\n",
    "\n",
    "finetune_dir = \"./dataset/finetuned_data/stratified_direct_allrole\"\n",
    "if not os.path.exists(finetune_dir):\n",
    "    os.makedirs(finetune_dir)\n",
    "\n",
    "for m in files.keys():\n",
    "    process_save(m, finetune_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### set up role_list and special_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "from copy import deepcopy\n",
    "\n",
    "ROLE_LIST = ['disease', 'country', 'province', 'city', 'country Code', 'province Code', 'city Code', 'virus', 'symptoms', 'victims']\n",
    "special_tokens = ['<|disease|>', '<|/disease|>', '<|country|>', '<|/country|>', '<|province|>', '<|/province|>', '<|city|>', '<|/city|>', \n",
    "            '<|country Code|>', '<|/country Code|>', '<|province Code|>', '<|/province Code|>', '<|city Code|>', '<|/city Code|>', '<|virus|>', \n",
    "            '<|/virus|>', '<|symptoms|>', '<|/symptoms|>', '<|victims|>', '<|/victims|>', '[None]', '[And]']\n",
    "\n",
    "# ROLE_LIST = ['disease', 'country', 'province', 'city', 'virus', 'symptoms', 'victims']\n",
    "# special_tokens = ['<|disease|>', '<|/disease|>', '<|country|>', '<|/country|>', '<|province|>', '<|/province|>', '<|city|>', '<|/city|>', \n",
    "#             '<|virus|>', '<|/virus|>', '<|symptoms|>', '<|/symptoms|>', '<|victims|>', '<|/victims|>', '[None]', '[And]']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "extraction example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = \"<|disease|> anthrax <|/disease|> <|country|> United States <|/country|> <|province|> Texas <|/province|> <|city|> Crockett County [And] Kinney County [And] Sutton County [And] Uvalde County [And] Val Verde County <|/city|> <|country Code|> 6252001 <|/country Code|> <|province Code|> 4736286 <|/province Code|> <|city Code|> 4738723 [And] 5519699 [And] 5524590 [And] 5531955 [And] 5532889 <|/city Code|> <|virus|> Bacillus anthracis <|/virus|> <|symptoms|> [None] <|/symptoms|> <|victims|> Animal <|/victims|>\"\n",
    "t2 = \"<|disease|> Salmonellosis <|/disease|> <|country|> United States <|/country|> <|province|> Minnesota  [And] Wisconsin  <|/province|> <|city|> [None] <|/city|> <|country Code|> 6252001 <|/country Code|> <|province Code|> 5037779 [And] 5279468 <|/province Code|> <|city Code|> [None] <|/city Code|> <|virus|> Salmonella <|/virus|> <|symptoms|> [None] <|/symptoms|> <|victims|> Human <|/victims|>\"\n",
    "def extract_outbreak(text):\n",
    "    output = []\n",
    "    tag_s = re.search('<\\|[^/>][^>]*\\|>', text)\n",
    "    while tag_s:\n",
    "        text = text[tag_s.end():]\n",
    "        r_type = tag_s.group()[2:-2]\n",
    "        if r_type in ROLE_LIST:\n",
    "            tag_e = re.search(f'<\\|/{r_type}\\|>', text)\n",
    "            if tag_e:\n",
    "                arg = text[:tag_e.start()].strip()\n",
    "                for a in arg.split(f' [And] '):\n",
    "                    for aa in a.split('[and]'):\n",
    "                        aa = aa.strip()\n",
    "                        if aa != '' and a != \"[None]\":\n",
    "                            output.append((aa.lower(), r_type))\n",
    "                text = text[tag_e.end():]\n",
    "        tag_s = re.search('<\\|[^/>][^>]*\\|>', text)\n",
    "    return output\n",
    "\n",
    "o1 = extract_outbreak(t1)\n",
    "o2 = extract_outbreak(t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('anthrax', 'disease'),\n",
       " ('united states', 'country'),\n",
       " ('texas', 'province'),\n",
       " ('crockett county', 'city'),\n",
       " ('kinney county', 'city'),\n",
       " ('sutton county', 'city'),\n",
       " ('uvalde county', 'city'),\n",
       " ('val verde county', 'city'),\n",
       " ('6252001', 'country Code'),\n",
       " ('4736286', 'province Code'),\n",
       " ('4738723', 'city Code'),\n",
       " ('5519699', 'city Code'),\n",
       " ('5524590', 'city Code'),\n",
       " ('5531955', 'city Code'),\n",
       " ('5532889', 'city Code'),\n",
       " ('bacillus anthracis', 'virus'),\n",
       " ('animal', 'victims')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load predicted results and construct database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_aliases = {}\n",
    "\n",
    "with open(\"dataset/synonym/city.json\") as f:\n",
    "    all_aliases[\"city\"] = json.load(f)\n",
    "\n",
    "with open(\"dataset/synonym/country.json\") as f:\n",
    "    all_aliases[\"country\"] = json.load(f)\n",
    "\n",
    "with open(\"dataset/synonym/disease.json\") as f:\n",
    "    all_aliases[\"disease\"] = json.load(f)\n",
    "\n",
    "with open(\"dataset/synonym/province.json\") as f:\n",
    "    all_aliases[\"province\"] = json.load(f)\n",
    "\n",
    "with open(\"dataset/synonym/virus.json\") as f:\n",
    "    all_aliases[\"virus\"] = json.load(f)\n",
    "\n",
    "with open(\"dataset/synonym/symptoms.json\") as f:\n",
    "    all_aliases[\"symptoms\"] = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make everything in small case\n",
    "for k, i in all_aliases.items():\n",
    "    for kk, ii in i.items():\n",
    "        all_aliases[k][kk] = [iii.lower() for iii in ii]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapped_aliases = {\"country\": {}, \"city\": {}, \"province\": {}, \"virus\": {}, \"disease\": {}, \"symptoms\": {}}\n",
    "searched = []\n",
    "\n",
    "# Replace 'your_username' with your GeoNames username\n",
    "\n",
    "# test_results = json.load(open(\"\"))\n",
    "# test_results = json.load(open(\"./predictions/direct_T5/test.pred.json\"))\n",
    "# test_results = json.load(open(\"./output/direct_T5base_withoutcode/20230306_190639/pred.test.json\"))\n",
    "# test_results = json.load(open(\"./output/direct_bart_base_withoutcode/20230409_221130/pred.test.json\"))\n",
    "# test_results = json.load(open(\"./output/stratified_direct_T5base_withoutcode/20230404_215037/pred.test.json\"))\n",
    "# test_results = json.load(open(\"./output/stratified_direct_bart_base_withoutcode/20230409_221725/pred.test.json\"))\n",
    "\n",
    "# test_results = json.load(open(\"./output/direct_T5base_allrole/20230228_155419/pred.test.json\"))\n",
    "# test_results = json.load(open(\"./output/direct_bart_base_allrole/20230512_212611/pred.test.json\"))\n",
    "test_results = json.load(open(\"./predictions/evaluation_galactica_new.json\"))\n",
    "\n",
    "# map synonyms\n",
    "for i in range(len(test_results)):\n",
    "    gs = extract_outbreak(test_results[i]['true_event']) #gold text\n",
    "    for idx, g_tuple in enumerate(gs):\n",
    "        if g_tuple[1] in all_aliases.keys():\n",
    "            value = g_tuple[0]\n",
    "            if value not in mapped_aliases[g_tuple[1]]:\n",
    "                # search for it\n",
    "                keys_to_search = list(all_aliases[g_tuple[1]].keys())\n",
    "                num_to_search = len(keys_to_search)\n",
    "                k = 0\n",
    "                while k < num_to_search:\n",
    "                    if value in all_aliases[g_tuple[1]][keys_to_search[k]]:\n",
    "                        mapped_aliases[g_tuple[1]][value] = all_aliases[g_tuple[1]][keys_to_search[k]]\n",
    "                        break\n",
    "                    k+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_results = json.load(open(\"./output/direct_T5base_allrole/20230228_155419/pred.test.json\"))\n",
    "# test_results = json.load(open(\"./output/direct_bart_base_allrole/20230512_212611/pred.test.json\"))\n",
    "test_results = json.load(open(\"./predictions/evaluation_opt_new.json\"))\n",
    "\n",
    "gold_test, gold_alias_test, pred_test, match_test = [], [], [], []\n",
    "gold_text_count = 0\n",
    "for i in range(len(test_results)):\n",
    "    gs = extract_outbreak(test_results[i]['true_event']) #gold text\n",
    "    gold_test.extend(gs)\n",
    "    alters = []\n",
    "    for idx, g_tuple in enumerate(gs):\n",
    "        if g_tuple[1] in mapped_aliases.keys():\n",
    "            value = g_tuple[0]\n",
    "            if value in mapped_aliases[g_tuple[1]]:\n",
    "                for alter in mapped_aliases[g_tuple[1]][value]:\n",
    "                    alters.append((alter, g_tuple[1]))\n",
    "    gs = gs + alters\n",
    "    ps = extract_outbreak(test_results[i]['generated_event']) #pred text\n",
    "\n",
    "    # ps = test_results[i]['pred attribute']\n",
    "    # gs = test_results[i]['gold attribute']\n",
    "\n",
    "    # this is attribute_all, entity pair level f1 score\n",
    "    if ps != []:\n",
    "        pred_test.extend(ps)\n",
    "        copy_gs = deepcopy(gs)\n",
    "        for p in ps:\n",
    "            if p in copy_gs:\n",
    "                copy_gs.remove(p)\n",
    "                match_test.append(p)\n",
    "\n",
    "    gold_alias_test.extend(gs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute F1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1447 1416 691\n",
      "total F1:  (0.47753973738769867, 0.4879943502824859, 0.4827104435906392)\n"
     ]
    }
   ],
   "source": [
    "def safe_div(num, denom):\n",
    "    if denom > 0:\n",
    "        return num / denom\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def compute_f1(predicted, gold, matched):\n",
    "    precision = safe_div(matched, predicted)\n",
    "    recall = safe_div(matched, gold)\n",
    "    f1 = safe_div(2 * precision * recall, precision + recall)\n",
    "    return precision, recall, f1\n",
    "    \n",
    "\n",
    "print(len(pred_test), len(gold_test), len(match_test))\n",
    "print(\"total F1: \", compute_f1(len(pred_test), len(gold_test), len(match_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Individual F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'disease': 66.89, 'country': 82.51, 'province': 49.54, 'city': 43.09, 'country Code': 32.24, 'province Code': 0.62, 'city Code': 0, 'virus': 52.99, 'symptoms': 54.68, 'victims': 94.31}\n",
      "66.89 & 82.51 & 49.54 & 43.09 & 32.24 & 0.62 & 0 & 52.99 & 54.68 & 94.31\n"
     ]
    }
   ],
   "source": [
    "classes = ['disease', 'country', 'province', 'city', 'country Code', 'province Code', 'city Code', 'virus', 'symptoms', 'victims']\n",
    "\n",
    "# Separate predicted and gold labels for each class\n",
    "pred_labels = {c: [] for c in classes}\n",
    "gold_labels = {c: [] for c in classes}\n",
    "match_labels = {c: [] for c in classes}\n",
    "for p in pred_test:\n",
    "    if p[1] in classes:\n",
    "        pred_labels[p[1]].append(p[0])\n",
    "for g in gold_test:\n",
    "    if g[1] in classes:\n",
    "        gold_labels[g[1]].append(g[0])\n",
    "for m in match_test:\n",
    "    if m[1] in classes:\n",
    "        match_labels[m[1]].append(m[0])\n",
    "\n",
    "\n",
    "# Calculate F1 score for each class\n",
    "F1s = {}\n",
    "for c in classes:\n",
    "    if gold_labels[c] and pred_labels[c]:\n",
    "        report = compute_f1(len(pred_labels[c]), len(gold_labels[c]), len(match_labels[c]))\n",
    "        F1s[c] = round(report[2]*100, 2)\n",
    "\n",
    "print(F1s)\n",
    "print(' & '.join([str(i) for i in list(F1s.values())]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6145363408521303\n"
     ]
    }
   ],
   "source": [
    "## F1 score without predicting code\n",
    "num_g, num_p, num_m = 0,0,0\n",
    "for c in ['disease', 'country', 'province', 'city', 'virus', 'symptoms', 'victims']:\n",
    "    num_g += len(gold_labels[c])\n",
    "    num_p += len(pred_labels[c])\n",
    "    num_m += len(match_labels[c])\n",
    "\n",
    "report = compute_f1(num_p, num_g, num_m)\n",
    "print(report[2])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing for DE-PPN (ONE-IE is for sentence-level)？\n",
    "Parallel Prediction for Document-level Event Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = {'train': './dataset/rand/train.csv', \n",
    "         'dev': './dataset/rand/dev.csv',\n",
    "         'test': './dataset/rand/test.csv'}\n",
    "\n",
    "import nltk\n",
    "# nltk.download('punkt')\n",
    "\n",
    "\n",
    "# For DE-PNN\n",
    "# data format\n",
    "# lists [doc_id, dict_keys(['sentences', 'ann_valid_mspans', 'ann_valid_dranges', 'ann_mspan2dranges', 'ann_mspan2guess_field', 'recguid_eventname_eventdict_list'])]\n",
    "# sentences: list of sentences e.g. [\"证券代码：300142证券简称：沃森生物公告编号：2016-072\", \"云南沃森生物技术股份有限公司关于股东解除股权质押的公告\", ... ]\n",
    "# mspan: list of entity text  e.g. [\"300142\", \"沃森生物\", ... ]\n",
    "# dranges: list of span position [sentence_index, start_index, end_index] e.g. [[0, 5, 11], [0, 16, 20], ... ] span_text = sentences[sentence_index][start_index:end_index]\n",
    "# mspan2dranges: dictionary map mspan text to drange e.g. {\"300142\": [0,5,11], \"沃森生物\": [0, 16,20], ...}\n",
    "# mspan2guess_field: dictionary map mspan text to role-type e.g. {\"300142\": \"StockCode\", \"沃森生物\": \"StockAbbr\", ...}\n",
    "# recguid_eventname_eventdict_list: list of events [event_1, event_2, ...], event_i: [id (start from 0), event_type (string), attributes ({role: text_span})]\n",
    "\n",
    "\n",
    "# 'disease', 'country', 'province', 'city', 'virus', 'symptoms', 'victims', 'Text']\n",
    "# span based method could not predict unseen word ..., check on Zihao's NER results\n",
    "import re\n",
    "import difflib\n",
    "\n",
    "\n",
    "def find_word_exact(sentences, query_word):\n",
    "    if query_word[-1] == \" \":\n",
    "        query_word = query_word[:-1]\n",
    "    for sentence_index, sentence in enumerate(sentences):\n",
    "        # pattern = r'\\b' + re.escape(query_word) + r'\\w*\\b'\n",
    "        pattern = r'\\b' + re.escape(query_word) + r'\\b'\n",
    "        matches = list(re.finditer(pattern, sentence, re.IGNORECASE))\n",
    "        result = []\n",
    "        if matches:\n",
    "            for match in matches:\n",
    "                start, end = match.span()\n",
    "                result.append([sentence_index, start, end])\n",
    "            return result\n",
    "    return None\n",
    "\n",
    "def find_word_partial(sentences, query_words):\n",
    "    if query_word[-1] == \" \":\n",
    "        query_word = query_word[:-1]\n",
    "    for sentence_index, sentence in enumerate(sentences):\n",
    "        for query_word in query_words.split(\" \"):\n",
    "            pattern = r'\\b' + re.escape(query_word) + r'\\w*\\b'\n",
    "            matches = list(re.finditer(pattern, sentence, re.IGNORECASE))\n",
    "            if matches:\n",
    "                start, end = matches[0].span()\n",
    "                return [sentence_index, start, end]\n",
    "    return None\n",
    "\n",
    "def find_word_sim(sentences, query_word, similarity_threshold=0.9):\n",
    "    if query_word[-1] == \" \":\n",
    "        query_word = query_word[:-1]\n",
    "    for sentence_index, sentence in enumerate(sentences):\n",
    "        words = re.findall(r'\\b\\w+\\b', sentence)\n",
    "        for word_index, word in enumerate(words):\n",
    "            similarity = difflib.SequenceMatcher(None, query_word.lower(), word.lower()).ratio()\n",
    "            if similarity >= similarity_threshold:\n",
    "                start = sentence.index(word)\n",
    "                end = start + len(word)\n",
    "                return [[sentence_index, start, end]]\n",
    "    return None\n",
    "\n",
    "\n",
    "for mode in ['train', 'dev', 'test']:\n",
    "    df = pd.read_csv(files[mode])\n",
    "    ids = df['Archive_id']\n",
    "    df = df[list(replace_to.keys())].rename(columns=replace_to).fillna(\"[None]\")\n",
    "    df = df.applymap(str.lower)\n",
    "    values = df.values.tolist()\n",
    "    keys = df.columns.tolist()\n",
    "\n",
    "    not_found = {i:[] for i in keys[:-1]}\n",
    "    all_events = []\n",
    "    for idx, sample_i in enumerate(values):\n",
    "        sentences = nltk.sent_tokenize(sample_i[-1].lower())\n",
    "        event_i = [0, \"outbreak\", {}]\n",
    "        valid_spans = []\n",
    "        span2role = {}\n",
    "        span2range = {}\n",
    "        ranges = []\n",
    "\n",
    "        found = []\n",
    "        for r in range(len(keys[:-1])):\n",
    "            if sample_i[r] == \"[none]\" or sample_i[r] == \"unknown\" or sample_i[r] == \"none\" or sample_i[r] == \"None\":\n",
    "                event_i[2][keys[r]] = []\n",
    "                continue\n",
    "            tar_i_r = sample_i[r].split(\"; \")\n",
    "            valid_i = []\n",
    "\n",
    "            for span_i in tar_i_r:\n",
    "                if span_i in found:\n",
    "                    continue\n",
    "                result = find_word_exact(sentences, span_i)\n",
    "                # if not result:\n",
    "                #     result = find_word_partial(sentences, span_i)\n",
    "                if not result:\n",
    "                    result = find_word_sim(sentences, span_i)\n",
    "                if not result:\n",
    "                    not_found[keys[r]].append([span_i, idx])\n",
    "                else:\n",
    "                    new_span = sentences[result[0][0]][result[0][1]:result[0][2]]\n",
    "                    found.append(span_i)\n",
    "                    valid_i.append(new_span)\n",
    "                    ranges.append(result)\n",
    "                    span2range[new_span] = result\n",
    "            \n",
    "            for span_i in valid_i:\n",
    "                span2role[span_i] = keys[r]\n",
    "                valid_spans.append(span_i)\n",
    "            \n",
    "            event_i[2][keys[r]] = valid_i\n",
    "\n",
    "        # find range\n",
    "        all_events.append([str(idx), {\"sentences\": sentences, \"spans\": valid_spans, \"ranges\": ranges, \"span2range\": span2range, \"span2role\": span2role, \"event\": [event_i]}])\n",
    "    \n",
    "    out_dir = \"DE-PPN/Data/epiai/\"\n",
    "    if not os.path.exists(out_dir):\n",
    "        os.makedirs(out_dir)\n",
    "    with open(out_dir+mode+\".json\", \"w\") as file:\n",
    "        json.dump(all_events, file, indent=4)\n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0',\n",
       " {'sentences': ['las vegas public health officials say dozens of people linked to a tuberculosis outbreak at a neonatal unit have tested positive for the disease.',\n",
       "   'the southern nevada health district reported on monday  that of the 977 people tested, 59 showed indications of the disease and 2 showed signs of being contagious.',\n",
       "   'dr joe iser, chief medical officer at the health district, says the report demonstrates the importance of catching tuberculosis early.',\n",
       "   \"health officials tested hundreds of babies, family members, and staff who were at summerlin hospital medical center's neonatal intensive care unit this past summer [2013], saying they wanted to take extra precautions after the death of a mother and her twin babies.\",\n",
       "   'they contacted the parents of about 140 babies who were at the unit between mid-may and mid-august [2013].'],\n",
       "  'spans': ['tuberculosis', 'nevada', 'las vegas'],\n",
       "  'ranges': [[[0, 67, 79]], [[1, 13, 19]], [[0, 0, 9]]],\n",
       "  'span2range': {'tuberculosis': [[0, 67, 79]],\n",
       "   'nevada': [[1, 13, 19]],\n",
       "   'las vegas': [[0, 0, 9]]},\n",
       "  'span2role': {'tuberculosis': 'diease',\n",
       "   'nevada': 'province',\n",
       "   'las vegas': 'city'},\n",
       "  'event': [[0,\n",
       "    'outbreak',\n",
       "    {'diease': ['tuberculosis'],\n",
       "     'country': [],\n",
       "     'province': ['nevada'],\n",
       "     'city': ['las vegas'],\n",
       "     'virus': [],\n",
       "     'symptoms': [],\n",
       "     'victims': []}]]}]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_events[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ebolavirus  infection',\n",
       " 'ghana ',\n",
       " 'greater accra region',\n",
       " 'accra ',\n",
       " 'ebolavirus ',\n",
       " '[none]',\n",
       " 'human',\n",
       " 'the american who was suspected to be carrying the ebolavirus at the nyaho clinic in accra is reported dead after blood testing on him revealed signs of the disease were glaring. the american, name withheld, died yesterday afternoon  while under surveillance at the infirmary. he arrived from guinea on sunday  and reported to the clinic for medical attention .\\n\\nthe initial tests run on the american, now deceased, according to the source, were inconclusive because the officials used the wrong reagent. the sources, who are medical practitioners, told joy news, \"the test should have taken noguchi  not more than 5 hours. but myjoyonline.com learnt from the public relations officer of the health ministry, tony goodman, that his outfit had requested some reagents from the kwame nkrumah university of science and technology [kumasi, ghana] to further help with the investigations. signals are that the blood samples could also be flown to [cdc] atlanta in the united states for further testing. early on, the head of disease surveillance of the ghana health service, dr. badu sarkodie, told joy news more work would be done later today  on the sample before a substantive conclusion could be determined.\\n\\nas a result, ghana\\'s health ministry is currently having a crunch meeting with stakeholders on the matter. the meeting is supposed to strategize on how to contain the deadly ebolavirus, should the disease break out in the country.']"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "for i, t, a in [all_events[0][1]['event']]:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "i, t, a = [0,\n",
    "           'outbreak',\n",
    "           {'disease': ['tuberculosis'],\n",
    "            'country': [],\n",
    "            'province': ['nevada'],\n",
    "            'city': ['las vegas'],\n",
    "            'virus': [],\n",
    "            'victims': []}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "i, t, a = all_events[0][1]['event']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_fin_data = json.load(open(\"DE-PPN/Data/sample_500.json\", \"r\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results:  [1, 0, 14]\n",
      "Results:  [0, 5, 11]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "    return None\n",
    "\n",
    "# Example usage:\n",
    "sentences = [\n",
    "    \"Some fungal infections can affect humans as well.\",\n",
    "    \"fungal disease are common in plants.\",\n",
    "    \"Bacterial infections disease can also be harmful.\",\n",
    "]\n",
    "\n",
    "words = \"fungal disease\"\n",
    "\n",
    "results = find_word_exact(sentences, words)\n",
    "print(\"Results: \", results)\n",
    "\n",
    "\n",
    "results = find_word_partial(sentences, words)\n",
    "print(\"Results: \", results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "## For OneIE\n",
    "# doc_id = Archive_id\n",
    "# sent_id; do we need sent_id? treat doc as a whole?\n",
    "# tokens = use tokenizer\n",
    "# pieces\n",
    "# token_lens\n",
    "# sentence\n",
    "# entity_mentions\n",
    "## in entity_mentions: id, text, entity_type, mention_type, entity_subtype, start, end\n",
    "# relation_mentions\n",
    "# event_mentions\n",
    "## in event mention: id, event_type, trigger {text, start, end},arguments [{entity_id, text, role}] \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('bartgen')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "badcac4507fde80d6540f7712b464a83cec89321f4aefbbd2af1c10224bd613b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
